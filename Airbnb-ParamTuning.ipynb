{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ASHEVILLE AIRBNB SENTIMENT ANALYSIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The purpose of this report is **to analyze customer reviews for Airbnb on Asheville, North Carolina, United States**. And act as a stepping stone **to know what the customers think of the service offered by Asheville's Airbnb, and this analysis could help to know if the hosts are providing good customer service or not**. The analysis progress would be separated on several notebook, and will cover from *data preprocessing, text preprocessing, topic modelling, visualization, model building, to model testing*. \n",
    "\n",
    "> This notebook specifically will only cover the **HYPERPARAMETER TUNING** part.\n",
    "\n",
    "> The dataset contains the **detailed review data for listings in Asheville, North Carolina** compiled on **08 November, 2020**. The data are from the **Inside Airbnb site**, it is sourced from publicly available information, from the Airbnb site. The data has been analyzed, cleansed and aggregated where appropriate to faciliate public discussion. More on this data, and other similar data refers to this [link](http://insideairbnb.com/get-the-data.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPORT LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data wrangling\n",
    "\n",
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "\n",
    "# data visualization\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# text processing\n",
    "\n",
    "import nltk\n",
    "import en_core_web_sm\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# modelling\n",
    "\n",
    "import joblib\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, StratifiedKFold, cross_val_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "# filter warning\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OVERVIEW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "\n",
    "df = pd.read_csv('asheville-reviews-tuning.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to check data summary\n",
    "\n",
    "def summary(df):\n",
    "    \n",
    "    columns = df.columns.to_list()\n",
    "    \n",
    "    dtypes = []\n",
    "    unique_counts = []\n",
    "    missing_counts = []\n",
    "    missing_percentages = []\n",
    "    total_counts = [df.shape[0]] * len(columns)\n",
    "\n",
    "    for col in columns:\n",
    "        dtype = str(df[col].dtype)\n",
    "        dtypes.append(dtype)\n",
    "        unique_count = df[col].nunique()\n",
    "        unique_counts.append(unique_count)\n",
    "        missing_count = df[col].isnull().sum()\n",
    "        missing_counts.append(missing_count)\n",
    "        missing_percentage = round((missing_count/df.shape[0]) * 100, 2)\n",
    "        missing_percentages.append(missing_percentage)\n",
    "\n",
    "    df_summary = pd.DataFrame({\n",
    "        \"column\": columns,\n",
    "        \"dtypes\": dtypes,\n",
    "        \"unique_count\": unique_counts,\n",
    "        \"missing_values\": missing_counts,\n",
    "        \"missing_percentage\": missing_percentages,\n",
    "        \"total_count\": total_counts,\n",
    "    })\n",
    "\n",
    "    return df_summary.sort_values(by=\"missing_percentage\", ascending=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>column</th>\n",
       "      <th>dtypes</th>\n",
       "      <th>unique_count</th>\n",
       "      <th>missing_values</th>\n",
       "      <th>missing_percentage</th>\n",
       "      <th>total_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>listing_id</td>\n",
       "      <td>int64</td>\n",
       "      <td>2042</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>172465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id</td>\n",
       "      <td>int64</td>\n",
       "      <td>172465</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>172465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>date</td>\n",
       "      <td>object</td>\n",
       "      <td>2904</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>172465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>reviewer_id</td>\n",
       "      <td>int64</td>\n",
       "      <td>157219</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>172465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>reviewer_name</td>\n",
       "      <td>object</td>\n",
       "      <td>16166</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>172465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>comments</td>\n",
       "      <td>object</td>\n",
       "      <td>169865</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>172465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>comments_cleaned</td>\n",
       "      <td>object</td>\n",
       "      <td>167552</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>172465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>comments_tokenized</td>\n",
       "      <td>object</td>\n",
       "      <td>161618</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>172465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>compound_score</td>\n",
       "      <td>float64</td>\n",
       "      <td>1841</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>172465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>sentiment</td>\n",
       "      <td>int64</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>172465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>topics</td>\n",
       "      <td>object</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>172465</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                column   dtypes  unique_count  missing_values  \\\n",
       "0           listing_id    int64          2042               0   \n",
       "1                   id    int64        172465               0   \n",
       "2                 date   object          2904               0   \n",
       "3          reviewer_id    int64        157219               0   \n",
       "4        reviewer_name   object         16166               0   \n",
       "5             comments   object        169865               0   \n",
       "6     comments_cleaned   object        167552               0   \n",
       "7   comments_tokenized   object        161618               0   \n",
       "8       compound_score  float64          1841               0   \n",
       "9            sentiment    int64             3               0   \n",
       "10              topics   object             5               0   \n",
       "\n",
       "    missing_percentage  total_count  \n",
       "0                  0.0       172465  \n",
       "1                  0.0       172465  \n",
       "2                  0.0       172465  \n",
       "3                  0.0       172465  \n",
       "4                  0.0       172465  \n",
       "5                  0.0       172465  \n",
       "6                  0.0       172465  \n",
       "7                  0.0       172465  \n",
       "8                  0.0       172465  \n",
       "9                  0.0       172465  \n",
       "10                 0.0       172465  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check summary\n",
    "\n",
    "summary(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Now on to parameter tuning process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HYPERPARAMETER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SGD Classifier param\n",
    "\n",
    "loss = ['hinge', 'log', 'modified_huber', 'squared_hinge', 'perceptron'] \n",
    "penalty = ['l1', 'l2', 'elasticnet'] \n",
    "alpha = [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000] \n",
    "learning_rate = ['constant', 'optimal', 'invscaling', 'adaptive'] \n",
    "eta0 = [1, 10, 100] \n",
    "\n",
    "sgd_param = {'loss' : loss, 'penalty': penalty, 'alpha' : alpha, 'learning_rate' : learning_rate, 'eta0' : eta0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to automate tuning process\n",
    "\n",
    "def cvparam(est, xtr, ytr):\n",
    "    scv = RandomizedSearchCV(estimator = est, param_distributions = sgd_param, cv=10, scoring = 'f1_macro')\n",
    "    result = scv.fit(xtr, ytr)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label the target\n",
    "\n",
    "df['sentiment'] = LabelEncoder().fit_transform(df['sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.39453716 0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.09858178]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.26343102]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# set the dependent\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features = 100)\n",
    "tf_idf = vectorizer.fit_transform(df['comments_cleaned']).toarray()\n",
    "print(tf_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(tf_idf, df['sentiment'], test_size=0.2, random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDClassifier(class_weight='balanced', random_state=101)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize model\n",
    "\n",
    "model_sgd = SGDClassifier(random_state = 42, class_weight='balanced')\n",
    "model_sgd.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score :  1 0.5268813769260572\n",
      "Parameter :  1 {'penalty': 'l1', 'loss': 'modified_huber', 'learning_rate': 'adaptive', 'eta0': 100, 'alpha': 0.0001}\n",
      "Score :  2 0.3254592396512974\n",
      "Parameter :  2 {'penalty': 'elasticnet', 'loss': 'modified_huber', 'learning_rate': 'optimal', 'eta0': 100, 'alpha': 100}\n",
      "Score :  3 0.405856978818368\n",
      "Parameter :  3 {'penalty': 'l2', 'loss': 'perceptron', 'learning_rate': 'optimal', 'eta0': 10, 'alpha': 0.1}\n"
     ]
    }
   ],
   "source": [
    "# search parameter\n",
    "\n",
    "for i in range(1,4):\n",
    "    cv_rfc = cvparam(model_sgd, X_train, y_train)\n",
    "    print('Score : ', i, cv_rfc.best_score_)\n",
    "    print('Parameter : ', i, cv_rfc.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize model\n",
    "\n",
    "first_model  = SGDClassifier(penalty='l1', loss='modified_huber', learning_rate='adaptive', eta0=100, alpha=0.0001, random_state=42, class_weight='balanced')\n",
    "second_model = SGDClassifier(penalty='elasticnet', loss='modified_huber', learning_rate='optimal', eta0=100, alpha=100, random_state=42, class_weight='balanced')\n",
    "third_model  = SGDClassifier(penalty='l2', loss='perceptron', learning_rate='optimal', eta0=10, alpha=0.1, random_state=42, class_weight='balanced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to automate model scoring \n",
    "\n",
    "def score(model, X_train, X_test, y_train, y_test):\n",
    "    model  = model.fit(X_train,y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    return print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.10      0.34      0.16       206\n",
      "           1       0.42      0.55      0.47      1357\n",
      "           2       0.98      0.96      0.97     32930\n",
      "\n",
      "    accuracy                           0.94     34493\n",
      "   macro avg       0.50      0.61      0.53     34493\n",
      "weighted avg       0.95      0.94      0.94     34493\n",
      "\n",
      "Model 1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       206\n",
      "           1       0.00      0.00      0.00      1357\n",
      "           2       0.95      1.00      0.98     32930\n",
      "\n",
      "    accuracy                           0.95     34493\n",
      "   macro avg       0.32      0.33      0.33     34493\n",
      "weighted avg       0.91      0.95      0.93     34493\n",
      "\n",
      "Model 2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.06      0.40      0.10       206\n",
      "           1       0.34      0.42      0.38      1357\n",
      "           2       0.98      0.94      0.96     32930\n",
      "\n",
      "    accuracy                           0.91     34493\n",
      "   macro avg       0.46      0.59      0.48     34493\n",
      "weighted avg       0.95      0.91      0.93     34493\n",
      "\n"
     ]
    }
   ],
   "source": [
    "models = [first_model, second_model, third_model]\n",
    "for i in range(3):\n",
    "    print('Model', i)\n",
    "    score(models[i], X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Looking at the results above, I think I'll go with the first parameter since the result in general is better than the rest of it. Therefore, I'll dump this model to proceed to the model testing part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model_sgd_tuned']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dump model with joblib\n",
    "\n",
    "joblib.dump(first_model, 'model_sgd_tuned')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REFERENCES \n",
    "\n",
    ">- https://www.knowledgehut.com/tutorials/machine-learning/hyperparameter-tuning-machine-learning\n",
    ">- https://towardsdatascience.com/how-to-make-sgd-classifier-perform-as-well-as-logistic-regression-using-parfit-cc10bca2d3c4"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
